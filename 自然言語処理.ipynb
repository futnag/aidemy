{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "トークン：自然言語を解析する際、文章の最小単位して扱われる文字や文字列のこと。\n",
    "タイプ：単語の種類を表す用語。\n",
    "文章：まとまった内容を表す文のこと。自然言語処理では一文を指すことが多い。\n",
    "文書：複数の文章から成るデータ一件分を指すことが多い。\n",
    "コーパス：文書または音声データにある種の情報を与えたデータ。\n",
    "シソーラス：単語の上位/下位関係、部分/全体関係、同義関係、類義関係などによって単語を分類し、体系づけた類語辞典・辞書。\n",
    "形態素：意味を持つ最小の単位。「食べた」という単語は、2つの形態素「食べ」と「た」に分解できる。\n",
    "単語：単一または複数の形態素から構成される小さな単位。\n",
    "表層：原文の記述のこと。\n",
    "原形：活用する前の記述のこと。\n",
    "特徴：文章や文書から抽出された情報のこと。\n",
    "辞書：自然言語処理では、単語のリストを指す。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "文章の単語分割の手法は大きく二つ存在し、 形態素解析 と Ngram があります。\n",
    "形態素 とは 意味を持つ最小の言語単位 のことであり、単語は一つ以上の形態素を持ちます。\n",
    "形態素解析 とは、辞書を利用して形態素に分割し、さらに形態素ごとに品詞などのタグ付け（情報の付与）を行うことを指します。\n",
    "\n",
    "一方で Ngram とは、 N文字ごとに単語を切り分ける 、または N単語ごとに文章を切り分ける 解析手法のことです。\n",
    "\n",
    "1文字、あるいは1単語ごとに切り出したものを モノグラム 、2文字（単語）ごとに切り出したものを バイグラム 、3文字（単語）ごとに切り出したものを トリグラム と呼びます。\n",
    "\n",
    "例えば「あいうえお」という文の文字のモノグラム・バイグラム・トリグラムを考えてみると以下のようになります。\n",
    "\n",
    "モノグラム：{あ, い, う, え, お}\n",
    "バイグラム：{あい, いう, うえ, えお}\n",
    "トリグラム：{あいう, いうえ, うえお}\n",
    "\n",
    "Ngram は形態素解析のように辞書や文法的な解説が不要であるため、 言語に関係なく 用いることができます。\n",
    "また Ngram は特徴抽出の漏れが発生しにくいメリットがありますが、ノイズが大きくなるデメリットがあります。 逆に形態素解析は辞書の性能差が生じてしまう代わりにノイズが少ないです。\n",
    "\n",
    "例えば、「東京都の世界一有名なIT企業」という少し長い文字列について検索する際、バイグラムによって文字列を分割し検索すると「京都」の企業がヒットする可能性が出てしまいます。 なぜなら「東京都」の文字バイグラムを列挙すると{東京, 京都}となるからです。\n",
    "形態素解析を適切に用いるとこのようなことは起こりませんが、性能の高い辞書を用意する必要があります。\n",
    "\n",
    "＜用語＞\n",
    "\n",
    "単語分割：文章を単語に分割すること\n",
    "品詞タグ付け：単語を品詞に分類して、タグ付けをする処理のこと\n",
    "形態素解析：形態素への分割と品詞タグ付けの作業をまとめたもの"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "形態素解析を行うにあたりあらかじめ形態素解析ツールが用意されており、日本語の形態素解析器として代表的なものにMeCabやjanomeなどがあります。\n",
    "\n",
    "MeCabやjanomeは辞書を参考に形態素解析を行います。\n",
    "ここではMecabの使い方を学習します。以下のMecabを使った形態素解析の実行例を見てください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MeCab\n",
    "# 分かち書き\n",
    "mecab = MeCab.Tagger(\"-Owakati\")\n",
    "print(mecab.parse(\"明日は晴れるでしょう。\"))\n",
    "\n",
    "\n",
    "# 形態素に分割\n",
    "mecab = MeCab.Tagger(\"-Ochasen\")\n",
    "print(mecab.parse(\"特急はくたか\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# janome\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "t = Tokenizer()# Tokenizerオブジェクトの作成\n",
    "\n",
    "tokens = t.tokenize(\"pythonの本を読んだ\")\n",
    "for token in tokens:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分かち書き\n",
    "tokens = t.tokenize(\"明日晴れたらいいのにな\", wakati=True)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 表層形を出力　＝　文中において文字列として実際に出現する形式のこと\n",
    "\n",
    "tokens = t.tokenize(\"pythonの本を読んだ\")\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.surface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 品詞のみを出力\n",
    "\n",
    "for token in tokens:\n",
    "    print(token.part_of_speech)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = t.tokenize(\"豚の肉を食べた\")\n",
    "\n",
    "word = []\n",
    "\n",
    "for token in tokens:\n",
    "    word_type = token.part_of_speech.split(\",\")[0]\n",
    "    print(token.part_of_speech.split(\",\"))\n",
    "\n",
    "    if word_type == \"名詞\" or word_type == \"動詞\":\n",
    "        word.append(token.surface)\n",
    "    \n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ngram\n",
    "\n",
    "Ngramのアルゴリズムは以下のgen_Ngramように書くことができます。\n",
    "単語のNgramを求めたい場合は、引数に単語と切り出したい数を入れます。\n",
    "文章のNgramを求めたい場合は、janomeのtokenize関数を用いて分かち書きのリストを作成し、\n",
    "その分かち書きのリストと切り出したい数を引数に入れます。 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from janome.tokenizer import Tokenizer\n",
    "t = Tokenizer()\n",
    "tokens = t.tokenize(\"太郎はこの本を二郎を見た女性に渡した。\", wakati=True)\n",
    "\n",
    "# 連続したN文字の単語は、(品詞を元に分割された数 - N + 1)個取ることができます。\n",
    "\n",
    "def gen_Ngram(words,N):\n",
    "    ngram = []\n",
    "    \n",
    "    for i in range(len(words) - N + 1):\n",
    "        cw = \"\"\n",
    "        for j in range(N):\n",
    "            cw += words[i + j]\n",
    "        ngram.append(cw)\n",
    "    \n",
    "    return ngram\n",
    "\n",
    "print(gen_Ngram(tokens, 2))\n",
    "print(gen_Ngram(tokens, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "正則化\n",
    "\n",
    "表記揺れを統一する処理のこと。\n",
    "全角を半角に統一や大文字を小文字に統一等、ルールベースで文字を変換する。\n",
    "\n",
    "＜用語＞\n",
    "\n",
    "表記揺れ：同じ文書の中で、同音・同義で使われるべき語句が異なって表記されていること\n",
    "正規化：表記揺れを防ぐためルールベースで文字をや数字を変換すること"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import neologdn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(neologdn.normalize(\"ﾊﾝｶｸｶﾅ\"))\n",
    "# => 'ハンカクカナ'\n",
    "print(neologdn.normalize(\"全角記号！？＠＃\"))\n",
    "# => '全角記号!?@#'\n",
    "print(neologdn.normalize(\"全角記号例外「・」\"))\n",
    "# => '全角記号例外「・」'\n",
    "print(neologdn.normalize(\"長音短縮ウェーーーーイ\"))\n",
    "# => '長音短縮ウェーイ'\n",
    "print(neologdn.normalize(\"チルダ削除ウェ~∼∾〜〰～イ\"))\n",
    "# => 'チルダ削除ウェイ'\n",
    "print(neologdn.normalize(\"いろんなハイフン˗֊‐‑‒–⁃⁻₋−\"))\n",
    "# => 'いろんなハイフン-'\n",
    "print(neologdn.normalize(\"　　　ＰＲＭＬ　　副　読　本　　　\"))\n",
    "# => 'PRML副読本'\n",
    "print(neologdn.normalize(\" Natural Language Processing \"))\n",
    "# => 'Natural Language Processing'\n",
    "print(neologdn.normalize(\"かわいいいいいいいいい\", repeat=6))\n",
    "# => 'かわいいいいいい'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自前でやるには nltk(Natural Language Toolkit) が便利\n",
    "\n",
    "import nltk\n",
    "t = \"iPhone, IPAD, MacBook\"\n",
    "\n",
    "def lower_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "lower_letters = lower_text(t)\n",
    "\n",
    "print(lower_letters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 正規化　数字の置き換え\n",
    "\n",
    "正規化では、数字の置き換えを行うことがあります。\n",
    "数字の置き換えを行う理由としては、 数値表現が多様で出現頻度が高い割には自然言語処理のタスクに役に立たない場合があるからです。\n",
    "たとえば、ニュース記事を「スポーツ」や「政治」のようなカテゴリに分類するタスクを考えましょう。\n",
    "この時、記事中には多様な数字表現が出現すると思いますが、カテゴリの分類にはほとんど役に立たないと考えられます。\n",
    "そのため、 数字を別の記号に置き換えて語彙数を減らしてしまいます。\n",
    "\n",
    "正規表現を使う方法\n",
    "\n",
    "re.sub(正規表現, 置換する文字列, 置換される文字列全体 [, 置換回数])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def normalize_number(text):\n",
    "    \n",
    "    replaced_text = re.sub(\"\\d\", \"!\", text)\n",
    "    return replaced_text\n",
    "\n",
    "\n",
    "replaced_text = normalize_number(\"終日は前日よりも39.03ドル(0.19%)高い。\")\n",
    "print(replaced_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "正規表現について\n",
    "\n",
    "正規表現\t意味\n",
    "\\d or [0-9]\t数字\n",
    "\\D or [^0-9]\t数字以外\n",
    "\\s or [\\t\\n\\r\\f\\v]\t空白\n",
    "\\w or [a-xA-Z0-9_]\t英数字\n",
    "\\W or [\\a-zA-Z0-9_]\t英数字以外\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "自然言語処理\n",
    "2.1.1\n",
    "文書のベクトル表現\n",
    "\n",
    "文書のベクトル表現 とは、 文書中に単語がどのように分布しているかをベクトルとして表現することです。\n",
    "例えば、「トマトときゅうりだとトマトが好き」という文は以下のようなベクトル表現に変換することができます。\n",
    "\n",
    "（が、きゅうり、好き、だと、と、トマト） = (1, 1, 1, 1, 1, 2)\n",
    "\n",
    "各単語の出現回数は表現されていますが、どこに出現したかの情報は失われています。\n",
    "つまり、構造や語順の情報が失われています。このようなベクトル表現方法を Bag of Words(BOW) と呼びます。\n",
    "\n",
    "ベクトル表現に変換する方法には、代表的なものが3つあります。\n",
    "\n",
    "・カウント表現：先ほどの例のように、文書中の各単語の出現数に着目する方法\n",
    "\n",
    "・バイナリ表現：出現頻度を気にせず、文章中に各単語が出現したかどうかのみに着目する方法\n",
    "\n",
    "・tf-idf表現：tf-idfという手法で計算された、文章中の各単語の重み情報を扱う方法\n",
    "\n",
    "一般的には、 td-idf が使われていますが、 文章数が多い場合においては計算に時間がかかる ため、 バイナリ表現やカウント表現 を用います。\n",
    "それぞれのベクトル表現に適材適所があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from janome.tokenizer import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = \"すもももももももものうち\"\n",
    "text2 = \"料理も景色もすばらしい\"\n",
    "text3 = \"私の趣味は写真撮影です\"\n",
    "\n",
    "t = Tokenizer()\n",
    "tokens1 = t.tokenize(text1, wakati=True)\n",
    "tokens2 = t.tokenize(text2, wakati=True)\n",
    "tokens3 = t.tokenize(text3, wakati=True)\n",
    "\n",
    "documents = [tokens1, tokens2, tokens3]\n",
    "# corporaを使い単語辞書を作成してください。\n",
    "dictionary = corpora.Dictionary(documents)\n",
    "\n",
    "# 各単語のidを表示してください\n",
    "print(\"各単語のid\")\n",
    "print(dictionary.token2id)\n",
    "\n",
    "# Bag of Wordsの作成してください\n",
    "bow_corpus = [dictionary.doc2bow(d) for d in documents]\n",
    "\n",
    "# (id, 出現回数)のリストが出力されます。\n",
    "print(\"(id, 出現回数)のリスト\")\n",
    "print(bow_corpus)\n",
    "\n",
    "print()\n",
    "# bow_corpusの内容をわかりやすく出力する\n",
    "texts = [text1, text2, text3]\n",
    "for i in range(len(bow_corpus)):\n",
    "    print(texts[i])\n",
    "    for j in range(len(bow_corpus[i])):\n",
    "        index = bow_corpus[i][j][0]\n",
    "        num = bow_corpus[i][j][1]\n",
    "        print(\"\\\"\", dictionary[index], \"\\\" が \" ,num, \"回\", end=\", \")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.1.3 BOW tf-idfによる重み付け（理論）\n",
    "\n",
    "「BOW_カウント表現」で行ったカウント表現では、文章を特徴付ける単語の出現回数を特徴量として扱いました。\n",
    "tf-idf は単語の出現頻度である tftf (Term frequency) と、その単語がどれだけ珍しいか（希少性）をしめす逆文書頻度 idfidf(Inverse Document Frequency) の 積で 表されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "実際にtf-idfを実装していきたいと思います。 \n",
    "scikit-learnが提供しているツールに TfidfVectorizer があり、それを用いて実装します。\n",
    "TfidfVectorizerは、「BOW　tf-idfによる重み付け（理論）」で説明した式を少し改良した形で実装されていますが、本質的な部分は同じです。\n",
    "\n",
    "vecs.toarray()のn行目が、もとの文書docsのn番目のベクトル表現に対応しています。\n",
    "\n",
    "コードの補足をします。\n",
    "\n",
    "vectorizer = TfidfVectorizer() で、ベクトル表現化を行う変換器を生成します。 use_idf=False にすると、tfのみの重み付けになります。\n",
    "TfidfVectorizerは、デフォルトで1文字の文字や文字列をトークンとして扱わない仕様になっているため、\n",
    "引数に token_pattern=\"(?u)\\\\b\\\\w+\\\\b\" を追加することで除外しないようにする必要があります。\n",
    "\"(?u)\\\\b\\\\w+\\\\b\"は、「1文字以上の任意の文字列」を表す正規表現ですが、深く理解する必要はありません。（正確には正規表現にエスケープシーケンスをつけたものです。）\n",
    "\n",
    "vectorizer.fit_transform() で、文書をベクトルに変換します。引数には、 空白文字によって分割された（分かち書きされた）文書からなる配列 を与えます。 toarrayによって出力をNumpyのndarray配列に変換できます。\n",
    "\n",
    "np.set_printoptions() は、numpy配列の表示のフォーマットを定める関数であり、precisionで有効数字を指定することができます。\n",
    "今回の例だと、有効数字二桁で表示されます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "docs = np.array([\n",
    "    \"リンゴ リンゴ\", \"リンゴ ゴリラ\", \"ゴリラ ラッパ\"\n",
    "])\n",
    "\n",
    "# ベクトル表現に変換してください。\n",
    "vectorizer = TfidfVectorizer(use_idf=True, token_pattern=\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "vecs = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(vecs.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.1.5 cos類似度\n",
    "\n",
    "これまでに文書を定量的に判断するために、文書のベクトル化をしてきました。\n",
    "そのベクトルを比較することにより、文書同士の類似度を解析することができます。\n",
    "ベクトルとベクトルがどれだけ近いものか示してくれるものに cos類似度 というものがあります。\n",
    "\n",
    "cos類似度は下記の数式で表され、ベクトルのなす角のコサイン(0~1)を表します。 そのためcos類似度は、二つのベクトルの方向が近いときに高い値を、反対の方向に向いている時に小さい値をとります。\n",
    "「1に近い場合は似ていて、0に近いときは似ていない」 ということをしっかりおさえておきましょう。\n",
    "\n",
    "実装すると以下のようになります。\n",
    "np.dot()は内積を表し、np.linalg.normはベクトルのノルム（ベクトルの長さ）を表しています。\n",
    "\n",
    "def cosine_similarity(v1, v2):\n",
    "    cos_sim = np.dot(v1, v2) / (np.linalg.norm(v1)*np.linalg.norm(v2))\n",
    "    return cos_sim\n",
    "\n",
    "つまり、２つのベクトルの内積を、両者のノルムの積でわればいい"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = np.array([\n",
    "    \"リンゴ リンゴ\", \"リンゴ ゴリラ\", \"ゴリラ ラッパ\"\n",
    "])\n",
    "vectorizer = TfidfVectorizer(use_idf=True, token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\")\n",
    "vecs = vectorizer.fit_transform(docs)\n",
    "vecs = vecs.toarray()\n",
    "\n",
    "# cos類似度を求める関数を定義してください。\n",
    "def cosine_similarity(v1, v2):\n",
    "    cos_sim = np.dot(v1, v2) / np.linalg.norm(v1) * np.linalg.norm(v2)\n",
    "    \n",
    "    return cos_sim\n",
    "\n",
    "# 類似度を比較してみましょう。\n",
    "print(\"%1.3F\" % cosine_similarity(vecs[0], vecs[1]))\n",
    "print(\"%1.3F\" % cosine_similarity(vecs[0], vecs[2]))\n",
    "print(\"%1.3F\" % cosine_similarity(vecs[1], vecs[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.2.1 Word2Vec\n",
    "\n",
    "前節では文書をベクトル表現として表していましたが、今回は単語をベクトル化します。\n",
    "単語をベクトルで表現すると、単語の意味の近さの数値化や同義語の探索などが行えます。\n",
    "\n",
    "最近の研究により、新しく生まれた単語をベクトル化するツールに Word2Vec というものがあります。\n",
    "簡潔に述べると 単語の意味や文法を捉えるために単語をベクトル表現化して次元を圧縮するツール です。\n",
    "日本人が日常的に使う語彙数は数万から数十万といわれていますが、Word2Vecでは各単語を200次元程度のベクトルとして表現できます。\n",
    "\n",
    "Word2Vecを用いると単語と単語の関係性を簡単に表現でき、\n",
    "「王様」 - 「男」+ 「女」 = 「女王」 \n",
    "「パリ」 - 「フランス」 + 「日本」 = 「東京」 \n",
    "のような 単語同士の 演算も可能となります。\n",
    "\n",
    "\n",
    "これから、 Word2Vec を用いて\"男\"という単語と関連性が高い文字列を調べていきます。\n",
    "フローは以下のようになります。対応するセッション名も同時に記しています。\n",
    "\n",
    "ニュース記事をテキストコーパスとして取り出し、文章とカテゴリーに分ける。 : 「globモジュール」、「with文」、「コーパスの呼び出し」\n",
    "取り出した文書を品詞ごとに分割し、リストにする。: 「janome(3)」\n",
    "Word2Vecでモデルを生成。: 「Word2Vec（実装）」\n",
    "男との関連性が高い語を調べる。: 「Word2Vec（実装）」\n",
    "    \n",
    "用語\n",
    "\n",
    "カウント表現：文書中の各単語の出現数\n",
    "BOW：文書のベクトル表現\n",
    "Word2Vec：単語をベクトル化するツール\n",
    "tf-idf：文章中の各単語の重み情報を扱う方法\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "globモジュール はファイルやディレクトリを操作するときに便利なモジュールであり、正規表現を用いてパスを指定できます。\n",
    "\n",
    "ファイル：文書、写真、音楽など、ユーザーが操作・管理する情報の最小単位。\n",
    "ディレクトリ：ファイルをまとめる入れ物のこと。\n",
    "パス：コンピュータ上でファイルやディレクトリの場所のこと。\n",
    "\n",
    "globモジュールが osモジュール （基本的にファイルやディレクトリを操作するときに用いるモジュール）と異なる点は、\n",
    "特殊な文字や文字列を用いて賢くファイルを検索できる点 です。\n",
    "例えば、アスタリスク * を用いて以下のように記述すると、 testディレクトリにあるtxtファイルを全て表示させることができます。\n",
    "\n",
    "lis = glob.glob(\"test/*.txt\")\n",
    "\n",
    "また、以下の例ようにして特殊な文字列を用いることができます。この例では、testディレクトリ以下のsample(数字).txtファイルを全て表示させることができます。\n",
    "\n",
    "lis = glob.glob(\"test/sample[0-9].txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# text/sports-watchの中にあるファイルを表示してください\n",
    "lis = glob.glob(\"./*\")\n",
    "print(lis)\n",
    "\n",
    "with open(lis[0], \"r\", encoding=\"utf-8\") as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "コーパス\n",
    "\n",
    "コーパスとは、文書または音声データにある種の情報を与えたデータのこと。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "def load_livedoor_news_corpus():\n",
    "    category = {\n",
    "        \"dokujo-tsushin\": 1,\n",
    "        \"it-life-hack\":2,\n",
    "        \"kaden-channel\": 3,\n",
    "        \"livedoor-homme\": 4,\n",
    "        \"movie-enter\": 5,\n",
    "        \"peachy\": 6,\n",
    "        \"smax\": 7,\n",
    "        \"sports-watch\": 8,\n",
    "        \"topic-news\":9\n",
    "    }\n",
    "\n",
    "    \n",
    "    docs  = [] # 全ての記事の文章をここに格納します。\n",
    "    labels = [] # docsに格納される記事の1〜9のカテゴリーを、ラベルとして扱います。\n",
    "\n",
    "\n",
    "    # 全てのカテゴリーのディレクトリについて実行します。\n",
    "    for c_name, c_id in category.items():\n",
    "        # {c_name}にcategory.items()から取得したカテゴリー名c_nameをformatメソッドを用いて埋め込みます。\n",
    "        \n",
    "        files = glob.glob(\"./text/{c_name}/{c_name}*.txt\".format(c_name=c_name))\n",
    "\n",
    "        # カテゴリーに属するファイル数（記事の数）を表示します。\n",
    "        print(\"category: \", c_name, \", \",  len(files))\n",
    "\n",
    "        # 各記事について、URL、 日付、タイトル、 本文の情報を以下のようにして取得します。\n",
    "        for file in files:\n",
    "            # with文を用いるため、close()は不要です。\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                # 改行文字で分割\n",
    "                lines = f.read().splitlines()\n",
    "                # 分割すると0番目にurl, 1番目に日付、2番目にタイトル、3番目以降に記事本文が記載されています。\n",
    "                url = lines[0]  \n",
    "                datetime = lines[1]  \n",
    "                subject = lines[2]\n",
    "                # 記事中の本文を1行にまとめてしまいます。\n",
    "                body = \"\".join(lines[3:])\n",
    "                # タイトルと本文をまとめてしまいます。\n",
    "                text = subject + body\n",
    "\n",
    "            docs.append(text)\n",
    "            labels.append(c_id)\n",
    "\n",
    "    return docs, labels\n",
    "    \n",
    "\n",
    "docs, labels = load_livedoor_news_corpus()\n",
    "\n",
    "print(\"\\nlabel: \", labels[0], \"\\ndocs:\\n\", docs[0])\n",
    "print(\"\\nlabel: \", labels[1000], \"\\ndocs:\\n\", docs[1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "下準備はできたので、本題の Word2Vec を説明していきます。\n",
    "Word2Vecを用いる時はgensimモジュールからimportします。\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "学習に使用するリスト（分かち書きされた文書）を Word2Vec関数 の引数とすることで、モデルを生成します。\n",
    "\n",
    "「BOW カウント」等で扱ったjanome.tokenizerを使って予め 分かち書き を行います。 分かち書きの際、各単語について品詞を調べます。\n",
    "日本語では、「名詞、動詞、形容詞、形容動詞」以外は単語の関連性の分析に使えないので、「名詞、動詞、形容詞、形容動詞」のみの分かち書きリストを作成します。\n",
    "\n",
    "Word2Vec は以下のようにして使います。\n",
    "\n",
    "model = word2vec.Word2Vec(リスト, size=a, min_count=b, window=c)\n",
    "# ただし、a, b, cは数字\n",
    "Word2Vecのよく使う引数は主に以下です。\n",
    "\n",
    "size ：ベクトルの次元数。\n",
    "window ：この数の前後の単語を、関連性のある単語と見なして学習を行う。\n",
    "min_count ：n回未満登場する単語を破棄。\n",
    "\n",
    "適切に学習が行われた後、 modelに対し .most_similar(positive=[\"単語\"]) のようにmost_similar()メソッドを用いるとその単語との類似度が高いものが出力されます。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "from gensim.models import word2vec\n",
    "\n",
    "# livedoor newsの読み込みと分類\n",
    "def load_livedoor_news_corpus():\n",
    "    category = {\n",
    "        \"dokujo-tsushin\": 1,\n",
    "        \"it-life-hack\":2,\n",
    "        \"kaden-channel\": 3,\n",
    "        \"livedoor-homme\": 4,\n",
    "        \"movie-enter\": 5,\n",
    "        \"peachy\": 6,\n",
    "        \"smax\": 7,\n",
    "        \"sports-watch\": 8,\n",
    "        \"topic-news\":9\n",
    "    }\n",
    "    docs  = []\n",
    "    labels = []\n",
    "\n",
    "    for c_name, c_id in category.items():\n",
    "        files = glob.glob(\"./text/{c_name}/{c_name}*.txt\".format(c_name=c_name))\n",
    "\n",
    "        text = \"\"\n",
    "        for file in files:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines() \n",
    "\n",
    "                #1,2行目に書いたあるURLと時間は関係ないので取り除きます。\n",
    "                url = lines[0]  \n",
    "                datetime = lines[1]  \n",
    "                subject = lines[2]\n",
    "                body = \"\".join(lines[3:])\n",
    "                text = subject + body\n",
    "\n",
    "            docs.append(text)\n",
    "            labels.append(c_id)\n",
    "\n",
    "    return docs, labels\n",
    "\n",
    "# 品詞を取り出し「名詞、動詞、形容詞、形容動詞」のリスト作成\n",
    "def tokenize(text):\n",
    "    t = Tokenizer()\n",
    "    tokens = t.tokenize(\",\".join(text))\n",
    "    word = []\n",
    "    for token in tokens:\n",
    "        part_of_speech = token.part_of_speech.split(\",\")[0]\n",
    " \n",
    "        if part_of_speech == \"名詞\":\n",
    "            word.append(token.surface)        \n",
    "        if part_of_speech == \"動詞\":        \n",
    "            word.append(token.surface)\n",
    "        if part_of_speech == \"形容詞\":\n",
    "            word.append(token.surface)        \n",
    "        if part_of_speech == \"形容動詞\":        \n",
    "            word.append(token.surface)            \n",
    "    return word\n",
    "\n",
    "# ラベルと文章に分類\n",
    "docs, labels = load_livedoor_news_corpus()\n",
    "sentences = tokenize(docs[0:100])  # データ量が多いため制限している\n",
    "\n",
    "\n",
    "# 以下に回答を作成してください\n",
    "#word2vec.Word2Vecの引数に関して、リストにはsentencesを指定し、size=200, min_count=20, window=15としてください\n",
    "model = word2vec.Word2Vec(sentences, size=200, min_count=20, window=15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.most_similar(positive=[\"男\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.3.1 Doc2Vec（1）\n",
    "\n",
    "Doc2Vec は、Word2Vecを応用した 文章をベクトル化する技術 です。\n",
    "「文書のベクトル表現」にてBOWで文章のベクトル化を勉強しましたが、BOWとの大きな違いは 文の語順 も特徴として考慮に入れることができる点です。\n",
    "\n",
    "文書のベクトル表現にて学んだBOWの欠点をおさらいすると、以下のようになります。\n",
    "\n",
    "単語の語順情報がない\n",
    "単語の意味の表現が苦手\n",
    "この二点の欠点をDoc2Vecは補っています。\n",
    "\n",
    "要点\n",
    "・BOW,Word2Vecは文書のベクトル表現やWord2Vecの章で学びました。\n",
    "・Doc2VecはBOWの、単語の語順情報がない、単語の意味の表現が苦手、という点を補ったものです。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "2.3.2 Doc2Vec（2）\n",
    "\n",
    "\n",
    "Doc2Vec を実装していきます。\n",
    "「コーパスの取り出し」にて作成したlivedoor newsコーパスのdocs[0],docs[1],docs[2],docs[3]の類似度を比較します。\n",
    "\n",
    "フローは以下のようになります。\n",
    "\n",
    "分かち書き\n",
    "TaggedDocument クラスのインスタンスを作成\n",
    "Doc2Vec でモデルの生成\n",
    "類似度の出力\n",
    "ポイント\n",
    "\n",
    "1.\n",
    "文章をjanomeのTokenizerを用い、分かち書きにします。\n",
    "\n",
    "2.\n",
    "TaggedDocumentの引数にwords=\"分かち書きされた各要素\", tags=[\"タグ\"]を与えると、 TaggedDocument クラスのインスタンスを作成できます。\n",
    "タグは文書のidのようなものです。\n",
    "TaggedDocumentをリストに格納し、これをDoc2Vecに渡します。\n",
    "\n",
    "3.\n",
    "モデルの学習は以下のように記述します。\n",
    "\n",
    "model = Doc2Vec(documents=リスト, min_count=1)\n",
    "min_count:最低この回数出現した単語のみを学習に使用\n",
    "\n",
    "4.\n",
    "類似度の出力は以下のように記述します。\n",
    "\n",
    "for i in range(4):\n",
    "    print(model.docvecs.most_similar(\"d\"+str(i)))\n",
    "\n",
    "問題にDoc2Vecの例を載せているので、解きながら使い方を覚えて行きましょう。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('d3', 0.9999418258666992), ('d1', 0.9999411106109619), ('d2', 0.9999375939369202)]\n",
      "[('d3', 0.9999696612358093), ('d2', 0.9999566078186035), ('d0', 0.9999411106109619)]\n",
      "[('d3', 0.9999698400497437), ('d1', 0.9999565482139587), ('d0', 0.9999375343322754)]\n",
      "[('d2', 0.9999698400497437), ('d1', 0.9999696016311646), ('d0', 0.9999417066574097)]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from janome.tokenizer import Tokenizer\n",
    "\n",
    "\n",
    "# livedoor newsの読み込みと分類\n",
    "def load_livedoor_news_corpus():\n",
    "    category = {\n",
    "        \"dokujo-tsushin\": 1,\n",
    "        \"it-life-hack\":2,\n",
    "        \"kaden-channel\": 3,\n",
    "        \"livedoor-homme\": 4,\n",
    "        \"movie-enter\": 5,\n",
    "        \"peachy\": 6,\n",
    "        \"smax\": 7,\n",
    "        \"sports-watch\": 8,\n",
    "        \"topic-news\":9\n",
    "    }\n",
    "    docs  = []\n",
    "    labels = []\n",
    "\n",
    "    for c_name, c_id in category.items():\n",
    "        files = glob.glob(\"./text/{c_name}/{c_name}*.txt\".format(c_name=c_name))\n",
    "\n",
    "        text = \"\"\n",
    "        for file in files:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines() \n",
    "\n",
    "                #1,2行目に書いたあるURLと時間は関係ないので取り除きます。\n",
    "                url = lines[0]  \n",
    "                datetime = lines[1]  \n",
    "                subject = lines[2]\n",
    "                body = \"\".join(lines[3:])\n",
    "                text = subject + body\n",
    "\n",
    "            docs.append(text)\n",
    "            labels.append(c_id)\n",
    "\n",
    "    return docs, labels\n",
    "\n",
    "docs, labels = load_livedoor_news_corpus()\n",
    "\n",
    "# Doc2Vecの処理\n",
    "token = [] # 各docsの分かち書きした結果を格納するリストです\n",
    "training_docs = [] # TaggedDocumentを格納するリストです\n",
    "\n",
    "for i in range(4):\n",
    "    \n",
    "    # docs[i] を分かち書きして、tokenに格納します\n",
    "    t = Tokenizer() \n",
    "    token.append(t.tokenize(docs[i], wakati=True))\n",
    "    \n",
    "    # TaggedDocument クラスのインスタンスを作成して、結果をtraining_docsに格納します\n",
    "    # タグは \"d番号\"とします\n",
    "    training_docs.append(TaggedDocument(words=token[i], tags=[\"d\" + str(i)]))\n",
    "\n",
    "# 以下に回答を作成してください\n",
    "#-------------------------------------------------------\n",
    "model = Doc2Vec(documents=training_docs, min_count=1)\n",
    "\n",
    "#-------------------------------------------------------\n",
    "\n",
    "for i in range(4):\n",
    "    print(model.docvecs.most_similar(\"d\"+str(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
