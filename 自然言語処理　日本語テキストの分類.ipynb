{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.1 日本語テキストの分類\n",
    "\n",
    "chapter1, chapter2で学んできたことを用いて 日本語テキストのカテゴリをランダムフォレスト で分類します。\n",
    "ここでもlivedoor newsを用います。ランダムフォレストに与えるデータはベクトル表現化したニュース記事で、9種類のカテゴリーに分類します。\n",
    "記事をベクトルで表すことにより、教師あり学習で学んだ方法をそのまま適用して記事の分類が行えます。\n",
    "\n",
    "このチャプターの学習フローは以下のようになります。\n",
    "\n",
    "・livedoor newsの読み込みと分類：「コーパスの取り出し」\n",
    "\n",
    "・データをトレイニングデータとテストデータに分割する：機械学習概論の「ホールドアウト法の理論と実践」\n",
    "\n",
    "・tf-idfでトレイニングデータとテストデータをベクトル化する：「BOW tf-idfによる重み付け（実装）」、「fit関数」\n",
    "\n",
    "・ランダムフォレストで学習：教師あり分類の「ランダムフォレスト」\n",
    "\n",
    "・実装：「コーパスのカテゴリをランダムフォレストで実装」\n",
    "\n",
    "・精度を上げる：「精度をあげる」"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.2 fit関数\n",
    "\n",
    "scikit-learn の変換系クラス(StandardScaler、Normalizer、TfidfVectorizer など)には、 fit(), fit_transform(), transform() などの関数があります。\n",
    "\n",
    "・fit()関数 ：渡されたデータの統計（最大値、最小値、平均、など）を取得して、メモリに保存。\n",
    "\n",
    "・transform()関数 ：fit()で取得した情報を用いてデータを書き換える。\n",
    "\n",
    "・fit_transform()関数 ：fit()の後にtransform()を実施する。\n",
    "\n",
    "fit()関数 はトレーニングデータセットからパラメーターを学習するために使用され、 tranform()関数 は　学習したパラメーターに基づいてデータが再形成されます。\n",
    "\n",
    "つまり、トレーニングデータの場合は fit_transform関数 を用い、\n",
    "テストデータの場合は,トレーニングデータの fit() の結果に基づくので、 transform()関数 を行う必要があります。\n",
    "fit()させるのはトレーニングデータです。テストデータにはfitさせてはいけません。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.3 コーパスのカテゴリをランダムフォレストで実装\n",
    "\n",
    "それではこれまで学んできたことを用いて、 livedoornewsコーパスのカテゴリをランダムフォレストで分類しましょう。\n",
    "「日本語テキストの分類」でも書いたようにフローは以下のようになります。\n",
    "\n",
    "・livedoor newsの読み込みと分類：「コーパスの取り出し」\n",
    "\n",
    "・データをトレイニングデータとテストデータに分割する：機械学習概論の「ホールドアウト法の理論と実践」\n",
    "\n",
    "・tf-idfでトレイニングデータとテストデータをベクトル化する：「BOW tf-idfによる重み付け（実装）」、「fit関数」\n",
    "\n",
    "・ランダムフォレストで学習：教師あり分類の「ランダムフォレスト」\n",
    "\n",
    "・実装：「コーパスのカテゴリをランダムフォレストで実装」\n",
    "\n",
    "・精度を上げる：「精度をあげる」\n",
    "\n",
    "トレーニングデータにはfit_transformを、テストデータにはtransformを使用しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.908535550653\n",
      "0.71302578019\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from janome.tokenizer import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def load_livedoor_news_corpus():\n",
    " \n",
    "    category = {\n",
    "        \"dokujo-tsushin\": 1,\n",
    "        \"it-life-hack\":2,\n",
    "        \"kaden-channel\": 3,\n",
    "        \"livedoor-homme\": 4,\n",
    "        \"movie-enter\": 5,\n",
    "        \"peachy\": 6,\n",
    "        \"smax\": 7,\n",
    "        \"sports-watch\": 8,\n",
    "        \"topic-news\":9\n",
    "    }\n",
    "    docs  = []\n",
    "    labels = []\n",
    "\n",
    "    for c_name, c_id in category.items():\n",
    "        files = glob.glob(\"./text/{c_name}/{c_name}*.txt\".format(c_name=c_name))\n",
    "\n",
    "#         text = \"\"\n",
    "        for file in files:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines() \n",
    "#                 url = lines[0]  \n",
    "#                 datetime = lines[1]  \n",
    "                subject = lines[2]\n",
    "                body = \"\".join(lines[3:])\n",
    "                text = subject + body\n",
    "\n",
    "            docs.append(text)\n",
    "            labels.append(c_id)\n",
    "\n",
    "    return docs, labels\n",
    "\n",
    "docs, labels = load_livedoor_news_corpus()\n",
    "\n",
    "# データをトレイニングデータとテストデータに分割する（機械学習概論 「ホールドアウト法の理論と実践」)\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(docs, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# tf-idfでトレイニングデータとテストデータをベクトル化する。(「fit関数」)\n",
    "#-------------------------------------------------------\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "train_matrix = vectorizer.fit_transform(train_data) # train_dataをベクトル化\n",
    "test_matrix = vectorizer.transform(test_data) # test_dataをベクトル化\n",
    "\n",
    "#-------------------------------------------------------\n",
    "\n",
    "\n",
    "# ランダムフォレストで学習（教師あり分類　「ランダムフォレスト」)\n",
    "clf = RandomForestClassifier(n_estimators=2)\n",
    "clf.fit(train_matrix, train_labels)\n",
    "\n",
    "\n",
    "\n",
    "# 精度の出力\n",
    "print(clf.score(train_matrix, train_labels))\n",
    "print(clf.score(test_matrix, test_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4.4 精度を上げる\n",
    "\n",
    "「コーパスのカテゴリをランダムフォレストで実装」で実装したプログラムの精度を上げる作業をしたいと思います。\n",
    "TfidfVectorizer() のパラメーターに tokenizer=関数 を設定すると、指定した関数でテキストを分割することができます。\n",
    "例えば、以下の関数を tokenizer= の引数とすると、「名詞、動詞、形容詞、形容動詞」のみの分割されたテキストを用います。\n",
    "そのため、分析に必要ない助詞・助動詞等がないので精度が上がる事になります。\n",
    "\n",
    "```python\n",
    "\n",
    "def tokenize(text):\n",
    "    tokens = t.tokenize(\",\".join(text))\n",
    "    noun = []\n",
    "    for token in tokens:\n",
    "        partOfSpeech = token.part_of_speech.split(\",\")[0]\n",
    " \n",
    "        if partOfSpeech == \"名詞\":\n",
    "            noun.append(token.surface)        \n",
    "        if partOfSpeech == \"動詞\":        \n",
    "            noun.append(token.surface)\n",
    "        if partOfSpeech == \"形容詞\":\n",
    "            noun.append(token.surface)        \n",
    "        if partOfSpeech == \"形容動詞\":        \n",
    "            noun.append(token.surface)            \n",
    "    return noun\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.912947564908\n",
      "0.728629579376\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from janome.tokenizer import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def load_livedoor_news_corpus():\n",
    " \n",
    "    category = {\n",
    "        \"dokujo-tsushin\": 1,\n",
    "        \"it-life-hack\":2,\n",
    "        \"kaden-channel\": 3,\n",
    "        \"livedoor-homme\": 4,\n",
    "        \"movie-enter\": 5,\n",
    "        \"peachy\": 6,\n",
    "        \"smax\": 7,\n",
    "        \"sports-watch\": 8,\n",
    "        \"topic-news\":9\n",
    "    }\n",
    "    docs  = []\n",
    "    labels = []\n",
    "\n",
    "    for c_name, c_id in category.items():\n",
    "        files = glob.glob(\"./text/{c_name}/{c_name}*.txt\".format(c_name=c_name))\n",
    "\n",
    "#         text = \"\"\n",
    "        for file in files:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines() \n",
    "#                 url = lines[0]  \n",
    "#                 datetime = lines[1]  \n",
    "                subject = lines[2]\n",
    "                body = \"\".join(lines[3:])\n",
    "                text = subject + body\n",
    "\n",
    "            docs.append(text)\n",
    "            labels.append(c_id)\n",
    "\n",
    "    return docs, labels\n",
    "\n",
    "docs, labels = load_livedoor_news_corpus()\n",
    "\n",
    "# データをトレイニングデータとテストデータに分割する（機械学習概論 「ホールドアウト法の理論と実践」)\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(docs, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# tf-idfでトレイニングデータとテストデータをベクトル化する。(「fit関数」)\n",
    "#-------------------------------------------------------\n",
    "\n",
    "# 品詞分類用の関数\n",
    "def tokenize(text):\n",
    "    tokens = t.tokenize(\",\".join(text))\n",
    "    noun = []\n",
    "    for token in tokens:\n",
    "    # 品詞を取り出し\n",
    "        partOfSpeech = token.part_of_speech.split(\",\")[0]\n",
    " \n",
    "        if partOfSpeech == \"名詞\":\n",
    "            noun.append(token.surface)        \n",
    "        if partOfSpeech == \"動詞\":        \n",
    "            noun.append(token.surface)\n",
    "        if partOfSpeech == \"形容詞\":\n",
    "            noun.append(token.surface)        \n",
    "        if partOfSpeech == \"形容動詞\":        \n",
    "            noun.append(token.surface)            \n",
    "    return noun\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenize)\n",
    "train_matrix = vectorizer.fit_transform(train_data) # train_dataをベクトル化\n",
    "test_matrix = vectorizer.transform(test_data) # test_dataをベクトル化\n",
    "\n",
    "#-------------------------------------------------------\n",
    "\n",
    "\n",
    "# ランダムフォレストで学習（教師あり分類　「ランダムフォレスト」)\n",
    "clf = RandomForestClassifier(n_estimators=2)\n",
    "clf.fit(train_matrix, train_labels)\n",
    "\n",
    "\n",
    "\n",
    "# 精度の出力\n",
    "print(clf.score(train_matrix, train_labels))\n",
    "print(clf.score(test_matrix, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.897166129306\n",
      "0.730664857531\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from janome.tokenizer import Tokenizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "def load_livedoor_news_corpus():\n",
    " \n",
    "    category = {\n",
    "        \"dokujo-tsushin\": 1,\n",
    "        \"it-life-hack\":2,\n",
    "        \"kaden-channel\": 3,\n",
    "        \"livedoor-homme\": 4,\n",
    "        \"movie-enter\": 5,\n",
    "        \"peachy\": 6,\n",
    "        \"smax\": 7,\n",
    "        \"sports-watch\": 8,\n",
    "        \"topic-news\":9\n",
    "    }\n",
    "    docs  = []\n",
    "    labels = []\n",
    "\n",
    "    for c_name, c_id in category.items():\n",
    "        files = glob.glob(\"./text/{c_name}/{c_name}*.txt\".format(c_name=c_name))\n",
    "\n",
    "#         text = \"\"\n",
    "        for file in files:\n",
    "            with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "                lines = f.read().splitlines() \n",
    "#                 url = lines[0]  \n",
    "#                 datetime = lines[1]  \n",
    "                subject = lines[2]\n",
    "                body = \"\".join(lines[3:])\n",
    "                text = subject + body\n",
    "\n",
    "            docs.append(text)\n",
    "            labels.append(c_id)\n",
    "\n",
    "    return docs, labels\n",
    "\n",
    "docs, labels = load_livedoor_news_corpus()\n",
    "\n",
    "# データをトレイニングデータとテストデータに分割する（機械学習概論 「ホールドアウト法の理論と実践」)\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(docs, labels, test_size=0.2, random_state=0)\n",
    "\n",
    "# tf-idfでトレイニングデータとテストデータをベクトル化する。(「fit関数」)\n",
    "#-------------------------------------------------------\n",
    "\n",
    "# 品詞分類用の関数\n",
    "def tokenize(text):\n",
    "    tokens = t.tokenize(\",\".join(text))\n",
    "    noun = []\n",
    "    for token in tokens:\n",
    "    # 品詞を取り出し\n",
    "        partOfSpeech = token.part_of_speech.split(\",\")[0]\n",
    " \n",
    "        if partOfSpeech == \"名詞\":\n",
    "            noun.append(token.surface)        \n",
    "        if partOfSpeech == \"動詞\":        \n",
    "            noun.append(token.surface)\n",
    "#         if partOfSpeech == \"形容詞\":\n",
    "#             noun.append(token.surface)        \n",
    "#         if partOfSpeech == \"形容動詞\":        \n",
    "#             noun.append(token.surface)            \n",
    "    return noun\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenize)\n",
    "train_matrix = vectorizer.fit_transform(train_data) # train_dataをベクトル化\n",
    "test_matrix = vectorizer.transform(test_data) # test_dataをベクトル化\n",
    "\n",
    "#-------------------------------------------------------\n",
    "\n",
    "\n",
    "# ランダムフォレストで学習（教師あり分類　「ランダムフォレスト」)\n",
    "clf = RandomForestClassifier(n_estimators=2)\n",
    "clf.fit(train_matrix, train_labels)\n",
    "\n",
    "\n",
    "\n",
    "# 精度の出力\n",
    "print(clf.score(train_matrix, train_labels))\n",
    "print(clf.score(test_matrix, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
