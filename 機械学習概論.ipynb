{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "テストデータとトレーニングデータの分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets,svm,cross_validation\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ホールド・アウト法\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train : (120, 4)\n",
      "y_train : (120,)\n",
      "X_test : (30, 4)\n",
      "y_test : (30,)\n"
     ]
    }
   ],
   "source": [
    "print (\"X_train :\", X_train.shape)\n",
    "print (\"y_train :\", y_train.shape)\n",
    "print (\"X_test :\", X_test.shape)\n",
    "print (\"y_test :\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-分割交差検証\n",
    "svc = svm.SVC(C=1, kernel=\"rbf\", gamma=0.001)\n",
    "scores = cross_validation.cross_val_score(svc, X, y, cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.86666667  0.96666667  0.83333333  0.96666667  0.93333333]\n",
      "平均スコア： 0.913333333333\n"
     ]
    }
   ],
   "source": [
    "print(scores)\n",
    "print(\"平均スコア：\", scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 性能評価\n",
    "学習済みモデルがどの程度良いものであるか判断する\n",
    "\n",
    "混同行列\n",
    "各テストデータに対するモデルの予測結果を、真陽性(True Positive)、真陰性(True Negative)、偽陽性(False Positive)、偽陰性(False Negative)の4つの観点で分類 をし、それぞれに当てはまる予測結果の個数をまとめた表です。\n",
    "\n",
    "・真陽性は陽性クラスと予測され結果も陽性クラスであった個数 \n",
    "・真陰性は陰性クラスと予測され結果も陰性クラスであった個数 \n",
    "・偽陽性は陽性クラスと予測されたが結果は陰性クラスであった個数 \n",
    "・偽陰性は陰性クラスと予測されたが結果は陽性クラスであった個数 \n",
    "\n",
    "       予測されたクラス\n",
    "        P        N\n",
    "実\n",
    "際 P  真陽性TP  偽陰性FN\n",
    "の     正解      不正解\n",
    "ク\n",
    "ラ N  偽陽性FP  真陰性TN\n",
    "ス     正解      不正解\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 1]\n",
      " [0 3]]\n"
     ]
    }
   ],
   "source": [
    "y_true = [0,0,0,1,1,1]\n",
    "y_pred = [1,0,0,1,1,1]\n",
    "\n",
    "confmat = confusion_matrix(y_true, y_pred)\n",
    "print(confmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.6666666666666666\n",
      "0.8\n"
     ]
    }
   ],
   "source": [
    "p = 2 / 2\n",
    "r = 2 / 3\n",
    "\n",
    "f = 2 * ((p * r) / (p + r))\n",
    "print(p)\n",
    "print(r)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 性能評価指標\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.75\n",
      "1.0\n",
      "0.828571428571\n",
      "F1: 0.857\n"
     ]
    }
   ],
   "source": [
    "y_true = [0,0,0,1,1,1]\n",
    "y_pred = [1,0,0,1,1,1]\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "\n",
    "f = f1_score(y_true, y_pred, average='macro')\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f)\n",
    "print(\"F1: %.3f\" % f1_score(y_true, y_pred))\n",
    "\n",
    "# \"\"\"\n",
    "# average : string, [None, ‘binary’ (default), ‘micro’, ‘macro’, ‘samples’, ‘weighted’]\n",
    "\n",
    "# This parameter is required for multiclass/multilabel targets. If None, the scores for each class are returned. Otherwise, this determines the type of averaging performed on the data:\n",
    "\n",
    "# 'binary':\n",
    "# Only report results for the class specified by pos_label. This is applicable only if targets (y_{true,pred}) are binary.\n",
    "\n",
    "# 'micro':\n",
    "# Calculate metrics globally by counting the total true positives, false negatives and false positives.\n",
    "\n",
    "# 'macro':\n",
    "# Calculate metrics for each label, and find their unweighted mean. This does not take label imbalance into account.\n",
    "\n",
    "# 'weighted':\n",
    "# Calculate metrics for each label, and find their average, weighted by support (the number of true instances for each label). This alters ‘macro’ to account for label imbalance; it can result in an F-score that is not between precision and recall.\n",
    "\n",
    "# 'samples':\n",
    "# Calculate metrics for each instance, and find their average (only meaningful for multilabel classification where this differs from accuracy_score).\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
